{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore the given dataset and try to understand its characteristics.\n",
    "\n",
    "First, let's load the data and check its size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (2061, 3)\n",
      "Evaluation size: (9000, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_excel(\"/home/adesoji/Data_dir/evaluation.xlsx\")\n",
    "train_df = pd.read_excel(\"/home/adesoji/Data_dir/train.xlsx\")\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Evaluation size:\", eval_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have `2061 samples` in the training set and `9000 samples` in the evaluation set. Next, let's check the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        reason  count\n",
      "0                            unable to use app    482\n",
      "1       good app for conducting online meeting    343\n",
      "2         good for watching movies and serials    236\n",
      "3                              app is not good    188\n",
      "4                        unable to play videos    174\n",
      "5          good app for connecting with people    138\n",
      "6                  want to cancel subscription    130\n",
      "7                              unable to login    130\n",
      "8          app is good to watch disney content    128\n",
      "9  getting ads despite paying for subscription    124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "df = eval_df\n",
    "\n",
    "# Get the count of each unique reason\n",
    "reason_counts = eval_df['reason'].value_counts()\n",
    "\n",
    "# Create a dataframe with the most common reasons\n",
    "most_common_reasons = pd.DataFrame({'reason': reason_counts.index, 'count': reason_counts.values})\n",
    "\n",
    "# Sort the dataframe by count in descending order\n",
    "most_common_reasons = most_common_reasons.sort_values('count', ascending=False)\n",
    "\n",
    "# Print the top 10 most common reasons\n",
    "print(most_common_reasons.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      reason  count\n",
      "0     good app for conducting online classes      1\n",
      "1417               unable to connect meeting      1\n",
      "1383             good for video conferencing      1\n",
      "1382             unable to download zoom app      1\n",
      "1381                want to download the app      1\n",
      "1380                      app is not working      1\n",
      "1379  good app for conducting online meeting      1\n",
      "1378     unable to switch virtual background      1\n",
      "1377                   video quality is poor      1\n",
      "1376                           want to login      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "df = train_df\n",
    "\n",
    "# Get the count of each unique reason\n",
    "reason_counts = train_df['reason'].value_counts()\n",
    "\n",
    "# Create a dataframe with the most common reasons\n",
    "most_common_reasons = pd.DataFrame({'reason': reason_counts.index, 'count': reason_counts.values})\n",
    "\n",
    "# Sort the dataframe by count in descending order\n",
    "most_common_reasons = most_common_reasons.sort_values('count', ascending=False)\n",
    "\n",
    "# Print the top 10 most common reasons\n",
    "print(most_common_reasons.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution:\n",
      " 1    1.0\n",
      "Name: label, dtype: float64\n",
      "Evaluation distribution:\n",
      " 0    0.666556\n",
      "1    0.333444\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train distribution:\\n\", train_df['label'].value_counts(normalize=True))\n",
    "print(\"Evaluation distribution:\\n\", eval_df['label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training set has only positive samples (label 1), while the evaluation set has both positive and negative samples. This means that we need to artificially generate negative samples during training to prevent the model from overfitting to the positive class \n",
    "\n",
    "Let's now look at some examples from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: this is an amazing app for online classes!but\n",
      "Reason: good app for conducting online classes\n",
      "Label: 1\n",
      "\n",
      "Text: very practical and easy to use\n",
      "Reason: app is user-friendly\n",
      "Label: 1\n",
      "\n",
      "Text: this app is very good for video conferencing.\n",
      "Reason: good for video conferencing\n",
      "Label: 1\n",
      "\n",
      "Text: i can not download this zoom app\n",
      "Reason: unable to download zoom app\n",
      "Label: 1\n",
      "\n",
      "Text: i am not able to download this app\n",
      "Reason: want to download the app\n",
      "Label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {train_df['text'][i]}\")\n",
    "    print(f\"Reason: {train_df['reason'][i]}\")\n",
    "    print(f\"Label: {train_df['label'][i]}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the text and reason are related to each other, and the label indicates whether they match or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our machine learning model, let's define a baseline approach that we can use to compare the performance of our model.\n",
    "\n",
    "A simple baseline approach is to check if the text and reason share any common words. If they do, we predict the label as 1, otherwise, we predict 0. Let's implement this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_predict(text, reason):\n",
    "    text_words = set(text.lower().split())\n",
    "    reason_words = set(reason.lower().split())\n",
    "    \n",
    "    if len(text_words.intersection(reason_words)) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate this baseline approach on the evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.34      0.48      5999\n",
      "           1       0.39      0.85      0.54      3001\n",
      "\n",
      "    accuracy                           0.51      9000\n",
      "   macro avg       0.61      0.60      0.51      9000\n",
      "weighted avg       0.68      0.51      0.50      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = eval_df['label'].values\n",
    "y_pred = [baseline_predict(text, reason) for text, reason in zip(eval_df['text'], eval_df['reason'])]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline approach achieves an F1 score of 0.48 on the negative class and 0.54 on the positive class, with an overall accuracy of 0.51 . We can use this as a starting point and try to improve the performance using machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 2061\n",
      "Evaluation dataset size: 9000\n",
      "\n",
      "Label distribution in the training dataset:\n",
      "1    1.0\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Label distribution in the evaluation dataset:\n",
      "0    0.666556\n",
      "1    0.333444\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'train.csv' and 'eval.csv' with the actual paths to your downloaded CSV files\n",
    "train_df \n",
    "eval_df \n",
    "\n",
    "# Dataset size\n",
    "train_size = train_df.shape[0]\n",
    "eval_size = eval_df.shape[0]\n",
    "\n",
    "print(f\"Training dataset size: {train_size}\")\n",
    "print(f\"Evaluation dataset size: {eval_size}\")\n",
    "\n",
    "# Label distribution in the training dataset\n",
    "train_label_counts = train_df['label'].value_counts(normalize=True)\n",
    "print(\"\\nLabel distribution in the training dataset:\")\n",
    "print(train_label_counts)\n",
    "\n",
    "# Label distribution in the evaluation dataset\n",
    "eval_label_counts = eval_df['label'].value_counts(normalize=True)\n",
    "print(\"\\nLabel distribution in the evaluation dataset:\")\n",
    "print(eval_label_counts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models are a class of models that can generate new data that is similar to the data it was trained on. Semantic similarity is a measure of the degree of equivalence in the underlying semantics of paired snippets of text. There are many generative models that can be used for semantic similarity. One such model is the **BERT-based semantic text similarity models**¹. Another model is the **Generative Adversarial Networks (GANs)**⁶. GANs are a class of generative models that can generate new data that is similar to the data it was trained on. They have shown great promise in a variety of applications, including image and speech synthesis, natural language processing, and drug discovery⁶. \n",
    "\n",
    "You can find more information about generative models and semantic similarity on **Google Scholar**³ and **GitHub**¹².\n",
    "\n",
    "Source: Conversation with Bing, 3/19/2023(1) GitHub - AndriyMulyar/semantic-text-similarity: an easy-to-use .... https://github.com/AndriyMulyar/semantic-text-similarity Accessed 3/19/2023.\n",
    "(2) Beyond Statistical Similarity: Rethinking Metrics for Deep Generative .... https://arxiv.org/abs/2302.02913 Accessed 3/19/2023.\n",
    "(3) Google Scholar. https://scholar.google.com/ Accessed 3/19/2023.\n",
    "(4) semantic-similarity · GitHub Topics · GitHub. https://github.com/topics/semantic-similarity Accessed 3/19/2023.\n",
    "(5) [PDF] Generative models for similarity-based classification | Semantic .... https://www.semanticscholar.org/paper/Generative-models-for-similarity-based-Cazzanti-Gupta/732cc3f31df533833ac0f6620fa0d3036cf38d6e Accessed 3/19/2023.\n",
    "(6) ‪Joshua B. Tenenbaum‬ - ‪Google Scholar‬. https://scholar.google.com/citations?user=rRJ9wTJMUB8C Accessed 3/19/2023.\n",
    "(7) Bilingual Generative Transformer for Semantic Sentence Embedding. https://aclanthology.org/2020.emnlp-main.122.pdf Accessed 3/19/2023.\n",
    "(8) nv-tlabs/semanticGAN_code - GitHub. https://github.com/nv-tlabs/semanticGAN_code Accessed 3/19/2023."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/semantic-text-similarity/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
